{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Medical Point-of-Care: Using Neural Networks to Diagnose Skin Cancer\n",
    "\n",
    "Author: \"Mark R. Tyrrell\"\n",
    "Module: CSC8635 Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The increased accuracy and agility of machine learning in recent years has pushed usability of the technology to the frontiers of many fields. This phenomenon is readily apparent in the field of medicine, where deep learning algorithms are being used for medical imaging analyses (Shen et al. 2017). The coincident emphasis on point-of-care testing (POCT) - replicating lab capacities at the clinical level - in the policy frameworks of many national health services further drives demand for these technologies. Applications of deep learning for POTC can improve patient outcomes by providing frontline clinicians with the tools necessary to make expert diagnoses without the need for specialist referrals. This in turn can decrease treatment lead-time, allowing for quicker intervention and better prognoses. \n",
    "\n",
    "This study examines the viability of developing and deploying a deep learning algorithm for a POCT application using publicly available data and commercial off-the-shelf (COTS) computing equipment. A convolutional neural network (CNN) will be used to analyse images of skin lesions in order to diagnosis the presence of skin cancer. The results will be compared with human expert assessment.\n",
    "\n",
    "## Background\n",
    "\n",
    "The prevalence of melanoma has increased steadily per capita since 1975 and continues to grow at 2.6 percent annually in the United States (Siegal et al. 2018; Higgens et al. 2014). Though low compared with other cancers, the mortality rate of melanoma is 8% in the United States (Siegal et al. 2018). This statistic highlights the relatively high survivability of this type of cancer, owing primarily to its visibility. In most cases malignant melanoma are identified early, and are subsequently curable. Conversely, treatment for metastasized melanoma is limited, resulting in a 5-year survival rate for stage-four melanoma of less than 15 percent (Higgens et al. 2014), underscoring the importance of early and accurate diagnoses. \n",
    "\n",
    "Malformed, mixed-pigmented and growing skin lesions are often readily apparent to patients and to primary healthcare providers while conducting physicals. Manual inspection of skin lesions using the Asymmetry, Border irregularity, Color variegation, Diameter >6 mm (ABCD) method for identifying potential malignancies has been the norm since 1985 and provides a first layer of screening with varying degrees of efficacy (Abbasi et al. 2004). If a malignancy is suspected, patients are referred to a determatologist for further screening. \n",
    "\n",
    "More advanced clinical-level diagnosis techniques such as dermatoscopy provide increased accuracy. However, this technology still requires the expertise of highly-trained specialists (Ebell 2008). Clearly there is a need to provide primary healthcare workers with easily applied, powerful diagnostic tools in order to identify malignant skin lesions prior to metastatic progression of the cancer. \n",
    "\n",
    "Computer vision is a logical candidate technology for automated skin cancer diagnosis. Deep learning algorithms have advanced steadily over the past two decades. CNNs can now match humans in simple object detection, and are being applied to many facets of medical imaging (Shen et al. 2017). The development of algorithms to detect melanomas and other skin cancers has been facilated by the recent release of the HAM10000 dataset (Tschandl et al. 2018). At the same time, open source machine learning libraries, cloud computing and smartphone cameras have made the training and deployment of CNNs accessible. Combined, these technologies and resources can be used to produce a cheap and accurate tool for diagnosing malignant skin lesions. This paper will demonstrate the simple development and deployment of a prototype diagnostic tool as proof-of-concept.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "In keeping with the goal of producing an affordable tool for diagnosing skin cancer, this project made exclusive use of open source toolkits. All data processing was accomplished using Python. The development of the CNN used the Python Keras library exclusively. Keras is a wrapper API on the Tensorflow library, and allows quick deployment of deep learning models with a user friendly syntax. \n",
    "\n",
    "Training of the CNN was conducted using ad hoc cloud computing resources. Microsoft Azure provides an array of different services including virtual machines (VMs) exclusively designed for machine learning tasks. This project utitlised an Azure NC6 Deep Learning VM with 6 virtual CPUs, 56 GB memory and a dedicated NVIDIA GPU. The total online time for the VM including setup time, code debugging and parameter optimisation was approximately 20 hours. The cost for this cloud-based resource was less than £20, make it highly applicable for this project.\n",
    "\n",
    "\n",
    "#### Data Understanding\n",
    "The HAM10000 dataset primarily consists of 10015 medium resolution images of seven different types of skin lesions, comprising 95 percent of lesions encountered in standard clinical practice (Tschandl et al. 2018). The images are preprocessed to ensure a minimal standard of contrast, focus and magnification of the lesion. The ground truth for each image is contained in an accompanying csv file along with metadata including the age and sex of the subject, the location of the lesion on the body, and the diagnosis method.\n",
    "\n",
    "The patient descriptor variables included in the metadata file such as age and sex could be used to contribute to the model. Age in particular has a direct positive correlation with increased prevalence of melanoma (Siegal et al. 2018). These correlations are well known to health care professionals and form the basis of screening regimes. As this project focuses on the use of innovative uses of image analysis for diagnoses, the model will exclude the metadata variables and only utilise the skin lesion images for model building.\n",
    "\n",
    "Table displays the quantity of each type of skin lesion represented in the dataset. While the dataset in total contains a reasonable quantity of data for training, the data are highly imbalanced. Melanoma comprises the majority of the samples (67%). This is problematic and can detrimentally affect classification performance (Buda et al. 2018). As the prior probability of any of the samples being Melanoma is very high at 67 percent, the model accuracy will be biased toward selecting Melanoma. The effect is clearly evidenced by observing the training process. The accuracy performance of the first epoch is no less than 66 percent in most trials. Additionally, the size of the dataset in this case further compounds the problem. The remaining 3310 samples are divided amongst the minority categories. Some categories contain less than 200 samples. This is insufficient for training a reliable classifier. \n",
    "\n",
    "\n",
    "**Table 1: HAM10000 Image data quantity by ground truth category**\n",
    "\n",
    "| Diagnostic Category | n |\n",
    "|:------|:-----:|\n",
    "|   Actinic Keratoses  | 327|\n",
    "|   Basal Cell Carcinoma  | 514|\n",
    "|   Benign Keratosis  | 1099|\n",
    "|   Dermatofibroma  | 115|\n",
    "|   Melanocytic Nevi  | 1113|\n",
    "|   Melanoma  | 6705|\n",
    "|   Vascular  | 142 |\n",
    "\n",
    "\n",
    "The imbalance in the dataset can be adjusted by dropping overrepresented samples, artificially increasing underrepresented samples, or a combination of both. Due to the relatively small size of the dataset, the only viable option in this case is oversampling the minority categories. The naive method for this involves creating duplicates of minority samples, ideally resulting in an even distribution of the categories. Further methods involve interpolation between samples of the same category to create new samples, for instance synthetic minority oversampling technique (SMOTE). \n",
    "\n",
    "Transfer learning provides yet another avenue for tackling the issues with the dataset. This is accomplished by using the convolutional and pooling layers of a CNN trained on another dataset, then adapting the fully connected layers to the target dataset and retraining. This technique is widely used and has proven to be effective in challenging contexts (Ramcharan et al. 2017). However, due to the excessive implementation requirements of these methods, both transfer learning and oversampling are beyond the scope of this study. \n",
    "\n",
    "Batch normalisation was tried on the first layer to mitigate the issues caused by imbalance. This method attempts to provide a normally distributed sample of the dataset to the input batch for each epoch. The algorithm can improve overall performance of CNNs as well as exhibit regularisation effects (Ioffe and Szegedy 2015). Surprisingly the model did not perform as well with batch normalisation enabled, and it was therefore removed from the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "The preparation of the data involves identifying the jpeg file paths from multiple folder locations, then merging the file paths vector with the metadata file. These file paths are then used to source the files, resizing and converting them to numpy arrays and inserting them into a new column in the metadata file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load standard data processing libraries\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# Assign project template directory\n",
    "data_dir = \"/Users/MT/Desktop/DS_Projects/CSC8635_ML_Project\"\n",
    "# Import metadata df\n",
    "meta = pd.read_csv(os.path.join(data_dir,'data/HAM10000_metadata.csv'))\n",
    "# Iterate through data_dir looking for jpg files and append to images_ls\n",
    "images_ls = []\n",
    "for dir,_,_ in os.walk(data_dir):\n",
    "    images_ls.extend(glob(os.path.join(dir,\"*.jpg\"))) \n",
    "\n",
    "# Convert images_ls to dataframe and assign variable name\n",
    "images_df = pd.DataFrame(images_ls)\n",
    "images_df.columns = ['path']\n",
    "\n",
    "# Extract image id from path for join with meta df\n",
    "images_df['image_id'] = images_df['path'].str[-16:-4]\n",
    "\n",
    "# Join image_df with meta on image id\n",
    "meta = pd.merge(meta, images_df, how='left', on=['image_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image data is provided in 600x450 pixel jpeg files. As higher-resolution imaging requires exponentially higher processing capability, the images were first resized to 100x75, maintaining the original aspect ratio. The resize operation requires considerable processing time, and therefore the output dataframe is cached as a *pickle* file to allow quick reloading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through images, resizing down to to 100x75 pixels, converting to array and inserting into new column\n",
    "#meta['image'] = meta['path'].map(lambda x: np.asarray(Image.open(x).resize((100,75))))\n",
    "\n",
    "# Cache result\n",
    "#meta.to_pickle(os.path.join(data_dir,\"cache/meta_cache.p\"))\n",
    "\n",
    "# Import cached metadata df (uncomment to implement)\n",
    "meta = pd.read_pickle(os.path.join(data_dir,'cache/meta_cache.p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the response and predictor variables are independently separated from the **meta** dataframe as vectors. The response vector **Y** contains the 7 skin lesion categories in string form. These categories are converted to integers, then into categories using *one-hot encoding*. \n",
    "\n",
    "The **X** vector contains the arrays of resized images of skin lesions. The array values are converted to floats, then centered by subtracting the mean of the vector, and normalised by dividing by the standard deviation of the vector.\n",
    "\n",
    "The vectors are then split twice using the **sklearn** *train-test-split* function. The first split separates allocates 10 percent of the data for post-training evaluation. The second split allocates 10 percent of the data for training cross-validation. A random seed is set for each split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract predictor variable (images) and labels as seperate vectors\n",
    "X=meta['image']\n",
    "Y=meta['dx']\n",
    "\n",
    "# Integer encode each response category and then one-hot encode\n",
    "label_encoder = LabelEncoder()\n",
    "Y = label_encoder.fit_transform(Y)\n",
    "Y = to_categorical(Y, num_classes = 7)\n",
    "\n",
    "# Iterate through images vector and normalise image array (divide by max RGB value = 255)\n",
    "X = np.asarray(meta['image'].tolist())\n",
    "X = X.astype('float32')\n",
    "X -= np.mean(X, axis=0)\n",
    "X /= np.std(X, axis=0)\n",
    "\n",
    "# Split test/train set for predictor and label variables\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1,random_state=10)\n",
    "\n",
    "# Split training set further for cross validation (NOT used for talos optimisation)\n",
    "X_train, X_validate, Y_train, Y_validate = train_test_split(X_train, Y_train, test_size = 0.1, random_state = 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Modeling\n",
    "The CNN was constructed using the Keras API for Tensorflow. The Keras package is optimised for GPU operation, providing relatively quick training time when implemented on a GPU-enabled machine. Hyper-parameter optimisation was accomplished using the Talos library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import keras utilities\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras import backend as K\n",
    "import itertools\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical \n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model consisted of 2 separate sets of 2 convolutional layers with 32 filter layers each. Each set was followed by a pooling layer and set for differing dropout levels for regularisation. The hyper-parameter optimisation process resulted in RELU selected as the activation function for these layers. \n",
    "\n",
    "The resulting model is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 75, 100, 3)        12        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 75, 100, 32)       896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 75, 100, 32)       9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 37, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 37, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 37, 50, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 37, 50, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 18, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 18, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 28800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               3686528   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 3,753,011\n",
      "Trainable params: 3,753,005\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set the CNN model \n",
    "# my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n",
    "input_shape = X_train.shape[1:]\n",
    "act = 'relu'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=input_shape))\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation=act,padding = 'Same',input_shape=input_shape))\n",
    "model.add(Conv2D(32,kernel_size=(3, 3), activation=act,padding = 'Same',))\n",
    "model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation=act,padding = 'Same'))\n",
    "model.add(Conv2D(64, (3, 3), activation=act,padding = 'Same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=act))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ADAM algorithm was selected for step optimisation, with an initial learning rate of 0.0001 and standard beta settings. Loss was evaluated using cross entropy. In addition to loss minimisation, Accuracy was used as the measure of model performance. ROC AUC has been shown to be a better measure in cases of imbalanced data, as it measures the ration of false postives to false negatives (Buda et al. 2018). However, there is no simple implementation in Keras.\n",
    "\n",
    "Automated learning rate reduction was also enabled using the Keras *ReduceLROnPlateau* callback function. This allowed the model to make adjustments to the learning rate during training when minimal change occured in the loss function over successive epochs. \n",
    "\n",
    "Additionally Keras allows the ability to make up for smaller dataset sizes by augmenting the input dataset. For instance, this function can rotate, zoom, shift or flip images. This allows for greater model generalisation to new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "# or 5 e-4\n",
    "\n",
    "# Start with no LR decay. Then optimise in subsequent models\n",
    "# Compile the model\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "# With data augmentation to prevent overfitting \n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to model fitting, a random seed was set in order to ensure reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed to ensure reproducability\n",
    "np.random.seed(10)      \n",
    "\n",
    "# Fit the model\n",
    "epochs = 100 \n",
    "batch_size = 9\n",
    "history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (X_val,Y_val),\n",
    "                              verbose = 1, steps_per_epoch=X_train.shape[0] // batch_size\n",
    "                              , callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selection of the hyper-parameters involved the use of Talos. This function allows automated training of models using different combinations of hyper-parameters. Due to the many permutations resulting from feeding just a few hyper-parameter ranges and the associated training time for each model, this tool must be used with caution. In practice, the utility works well following initial rough manual selection of learning rate, epochs and batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import talos\n",
    "import talos as ta\n",
    "\n",
    "# Define model inside function to be called by ta.Scan\n",
    "def rand_search(X_train, Y_train, X_val, Y_val, params):\n",
    "    conv_dropout = float(params['conv_dropout'])\n",
    "    dense1_neuron = int(params['dense1_neuron'])\n",
    "    \n",
    "    model = Sequential()\n",
    "#    model.add(BatchNormalization(input_shape=X_train.shape[1:]))\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation=params['activation']))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(conv_dropout))\n",
    "\n",
    "    model.add(BatchNormalization(input_shape=X_train.shape[1:]))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation=params['activation']))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(conv_dropout))\n",
    "\n",
    "    # Extra layers not used\n",
    "#    model.add(BatchNormalization(input_shape=X_train.shape[1:]))\n",
    "#    model.add(Conv2D(256, (5, 5), padding='same', activation=params['activation']))\n",
    "#    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "#    model.add(Dropout(conv_dropout))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dense1_neuron, activation=params['activation']))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.summary()\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Fit model\n",
    "    out = model.fit(\n",
    "        x, y, epochs=50, \n",
    "        batch_size=10, \n",
    "        verbose=1,\n",
    "        validation_data=[x_val, y_val],\n",
    "        callbacks=[learning_rate_reduction]\n",
    "    )\n",
    "    return out, model\n",
    "\n",
    "# Set ranges for hyper-parameter optimisation\n",
    "para = {\n",
    "    'dense1_neuron': [128, 256],\n",
    "    'activation': ['relu', 'elu'],\n",
    "    'conv_dropout': [0.2, 0.4, 0.5]\n",
    "}\n",
    "\n",
    "# Start scan using input data (validation sets created automatically)\n",
    "scan_results = ta.Scan(X_train, Y_train, para, rand_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Following training of the model using  The results show that even this simple implementation of a CNN can be an effect diagnostic tool for skin cancer. \n",
    "\n",
    "The model \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "How successful has it been? Provide evidence, using appropriate evaluation methodologies, and comment on the strengths/weaknesses of your evidence in answering this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_test, Y_test, verbose=1)\n",
    "loss_val, acc_val = model.evaluate(X_val, Y_val, verbose=1)\n",
    "print(\"Validation: accuracy = %f  ;  loss_v = %f\" % (acc_val, loss_val))\n",
    "print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n",
    "\n",
    "# Save model\n",
    "model.save(os.path.join(data_dir,\"cnn.hdf5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(os.path.join(data_dir,\"cnn_81_21JAN.hdf5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_test, Y_test, verbose=1)\n",
    "loss_val, acc_val = model.evaluate(X_val, Y_val, verbose=1)\n",
    "print(\"Validation: accuracy = %f  ;  loss_v = %f\" % (acc_val, loss_val))\n",
    "print(\"Test: accuracy = %f  ;  loss = %f\" % (acc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(history.history['acc'])\n",
    "hist_df.columns = ['acc']\n",
    "hist_df['loss'] = pd.DataFrame(history.history['loss'])\n",
    "hist_df['val_acc'] = pd.DataFrame(history.history['val_acc'])\n",
    "hist_df['val_loss'] = pd.DataFrame(history.history['val_loss'])\n",
    "hist_df['lr'] = pd.DataFrame(history.history['lr'])\n",
    "hist_df['epoch'] = range(10)\n",
    "\n",
    "from ggplot import *\n",
    "\n",
    "ggplot(aes(x='epoch', y='acc'), data=hist_df) +\\\n",
    "    geom_line() +\\\n",
    "    xlab(\"Epoch\") + ylab(\"Accuracy (%)\") + ggtitle(\"Training Accuracy\") +\\\n",
    "    geom_line(aes(y='val_acc'), color='red')\n",
    "\n",
    "ggplot(aes(x='epoch', y='loss'), data=hist_df) +\\\n",
    "    geom_line() +\\\n",
    "    xlab(\"Epoch\") + ylab(\"Loss (%)\") + ggtitle(\"Training Loss\") +\\\n",
    "    geom_line(aes(y='val_loss'), color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "What are the future implications for work in this area? If appli- cable, which areas of extension work are now possible due to the foundational work you have performed in this project?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "A brief reflection on your personal and professional learning in undertaking this project. Here you can comment on how you found the process, what you learned about the technologies and methodologies you used, which aspects you found most diffi- cult/straightforward, and any conclusions which will inform the way you undertake similar projects in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\n",
    "Shen et al. 2017\n",
    "@article{doi:10.1146/annurev-bioeng-071516-044442,\n",
    "author = {Shen, Dinggang and Wu, Guorong and Suk, Heung-Il},\n",
    "title = {Deep Learning in Medical Image Analysis},\n",
    "journal = {Annual Review of Biomedical Engineering},\n",
    "volume = {19},\n",
    "number = {1},\n",
    "pages = {221-248},\n",
    "year = {2017},\n",
    "doi = {10.1146/annurev-bioeng-071516-044442},\n",
    "    note ={PMID: 28301734},\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Higgens et al. 2014\n",
    "Point of care cutaneous imaging technology in melanoma screening and mole mapping\n",
    "H. William Higgins II, Kachiu C. Lee, David J. Leffell\n",
    "F1000Prime Rep. 2014; 6: 34. Published online 2014 May 6. doi: 10.12703/P6-34\n",
    "\n",
    "\n",
    "Siegal et al. 2018\n",
    "Cancer statistics, 2018\n",
    "\n",
    "Abbasi et al. 2004\n",
    "Early Diagnosis of Cutaneous Melanoma\n",
    "Revisiting the ABCD Criteria\n",
    "Naheed R. Abbasi, MPH, MD; Helen M. Shaw, PhD; Darrell S. Rigel, MD; et al\n",
    "\n",
    "Ebell 2008\n",
    "Clinical Diagnosis of Melanoma\n",
    "MARK EBELL, MD, MS, University of Georgia, Athens, Georgia\n",
    "Am Fam Physician. 2008 Nov 15;78(10):1205-1208.\n",
    "\n",
    "Tschandl et al. 2018\n",
    "Data Descriptor: The HAM10000 dataset, a large collection of multi-source dermatoscopic images\n",
    "of common pigmented skin lesions\n",
    "Philipp Tschandl1, Cliff Rosendahl2 & Harald Kittler1\n",
    "\n",
    "\n",
    "@misc{chollet2015keras,\n",
    "  title={Keras},\n",
    "  author={Chollet, Fran\\c{c}ois and others},\n",
    "  year={2015},\n",
    "  howpublished={\\url{https://keras.io}},\n",
    "}\n",
    "\n",
    "Buda et al. 2018\n",
    "Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance problem in convolutional neural networks. Neural Networks, 106:249–259, 2018.\n",
    "\n",
    "Ramcharan et al. 2017\n",
    "Amanda RamcharanKelsee BaranowskiKelsee BaranowskiPeter Charles McCloskeyPeter Charles McCloskeyShow all 6 authorsDavid P. Hughes\n",
    "Using Transfer Learning for Image-Based Cassava Disease Detection\n",
    "June 2017Frontiers in Plant Science 8\n",
    "\n",
    "Ioffe and Szegedy 2015\n",
    "Sergey Ioffe, Christian Szegedy\n",
    "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds36]",
   "language": "python",
   "name": "conda-env-ds36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
